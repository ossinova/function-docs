{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"function-docs Welcome and hello world!","title":"Mack"},{"location":"#function-docs","text":"Welcome and hello world!","title":"function-docs"},{"location":"reference/SUMMARY/","text":"functions dataframe_functions dataframe_validator","title":"API Docs"},{"location":"reference/functions/","text":"","title":"Index"},{"location":"reference/functions/dataframe_functions/","text":"column_to_list(df, col_name) Collect column to list of values. Parameters: Name Type Description Default df DataFrame Input DataFrame required col_name str Column to collect required Returns: Type Description List[Any] List of values Source code in functions/dataframe_functions.py def column_to_list(df: DataFrame, col_name: str) -> List[Any]: \"\"\"Collect column to list of values. :param df: Input DataFrame :type df: pyspark.sql.DataFrame :param col_name: Column to collect :type col_name: str :return: List of values :rtype: List[Any] \"\"\" return [x[col_name] for x in df.select(col_name).collect()] print_athena_create_table(df, athena_table_name, s3location) Generates the Athena create table statement for a given DataFrame Parameters: Name Type Description Default df DataFrame The pyspark.sql.DataFrame to use required athena_table_name str The name of the athena table to generate required s3location str The S3 location of the parquet data required Returns: Type Description None None Source code in functions/dataframe_functions.py def print_athena_create_table( df: DataFrame, athena_table_name: str, s3location: str ) -> None: \"\"\"Generates the Athena create table statement for a given DataFrame :param df: The pyspark.sql.DataFrame to use :param athena_table_name: The name of the athena table to generate :param s3location: The S3 location of the parquet data :return: None \"\"\" fields = df.schema print(f\"CREATE EXTERNAL TABLE IF NOT EXISTS `{athena_table_name}` ( \") for field in fields.fieldNames()[:-1]: print(\"\\t\", f\"`{fields[field].name}` {fields[field].dataType.simpleString()}, \") last = fields[fields.fieldNames()[-1]] print(\"\\t\", f\"`{last.name}` {last.dataType.simpleString()} \") print(\")\") print(\"STORED AS PARQUET\") print(f\"LOCATION '{s3location}'\\n\") show_output_to_df(show_output, spark) Show output as spark DataFrame Parameters: Name Type Description Default show_output str String representing output of 'show' command in spark required spark SparkSession SparkSession object required Returns: Type Description Dataframe DataFrame object containing output of a show command in spark Source code in functions/dataframe_functions.py def show_output_to_df(show_output: str, spark: SparkSession) -> DataFrame: \"\"\"Show output as spark DataFrame :param show_output: String representing output of 'show' command in spark :type show_output: str :param spark: SparkSession object :type spark: SparkSession :return: DataFrame object containing output of a show command in spark :rtype: Dataframe \"\"\" l = show_output.split(\"\\n\") ugly_column_names = l[1] pretty_column_names = [i.strip() for i in ugly_column_names[1:-1].split(\"|\")] pretty_data = [] ugly_data = l[3:-1] for row in ugly_data: r = [i.strip() for i in row[1:-1].split(\"|\")] pretty_data.append(tuple(r)) return spark.createDataFrame(pretty_data, pretty_column_names) to_list_of_dictionaries(df) Convert a Spark DataFrame to a list of dictionaries. Parameters: Name Type Description Default df DataFrame The Spark DataFrame to convert. required Returns: Type Description List[Dict[str, Any]] A list of dictionaries representing the rows in the DataFrame. Source code in functions/dataframe_functions.py def to_list_of_dictionaries(df: DataFrame) -> List[Dict[str, Any]]: \"\"\"Convert a Spark DataFrame to a list of dictionaries. :param df: The Spark DataFrame to convert. :type df: :py:class:`pyspark.sql.DataFrame` :return: A list of dictionaries representing the rows in the DataFrame. :rtype: List[Dict[str, Any]] \"\"\" return list(map(lambda r: r.asDict(), df.collect())) two_columns_to_dictionary(df, key_col_name, value_col_name) Collect two columns as dictionary when first column is key and second is value. Parameters: Name Type Description Default df DataFrame Input DataFrame required key_col_name str Key-column required value_col_name str Value-column required Returns: Type Description Dict[str, Any] Dictionary with values Source code in functions/dataframe_functions.py def two_columns_to_dictionary( df: DataFrame, key_col_name: str, value_col_name: str ) -> Dict[str, Any]: \"\"\"Collect two columns as dictionary when first column is key and second is value. :param df: Input DataFrame :type df: pyspark.sql.DataFrame :param key_col_name: Key-column :type key_col_name: str :param value_col_name: Value-column :type value_col_name: str :return: Dictionary with values :rtype: Dict[str, Any] \"\"\" k, v = key_col_name, value_col_name return {x[k]: x[v] for x in df.select(k, v).collect()}","title":"Dataframe functions"},{"location":"reference/functions/dataframe_functions/#functions.dataframe_functions.column_to_list","text":"Collect column to list of values. Parameters: Name Type Description Default df DataFrame Input DataFrame required col_name str Column to collect required Returns: Type Description List[Any] List of values Source code in functions/dataframe_functions.py def column_to_list(df: DataFrame, col_name: str) -> List[Any]: \"\"\"Collect column to list of values. :param df: Input DataFrame :type df: pyspark.sql.DataFrame :param col_name: Column to collect :type col_name: str :return: List of values :rtype: List[Any] \"\"\" return [x[col_name] for x in df.select(col_name).collect()]","title":"column_to_list()"},{"location":"reference/functions/dataframe_functions/#functions.dataframe_functions.print_athena_create_table","text":"Generates the Athena create table statement for a given DataFrame Parameters: Name Type Description Default df DataFrame The pyspark.sql.DataFrame to use required athena_table_name str The name of the athena table to generate required s3location str The S3 location of the parquet data required Returns: Type Description None None Source code in functions/dataframe_functions.py def print_athena_create_table( df: DataFrame, athena_table_name: str, s3location: str ) -> None: \"\"\"Generates the Athena create table statement for a given DataFrame :param df: The pyspark.sql.DataFrame to use :param athena_table_name: The name of the athena table to generate :param s3location: The S3 location of the parquet data :return: None \"\"\" fields = df.schema print(f\"CREATE EXTERNAL TABLE IF NOT EXISTS `{athena_table_name}` ( \") for field in fields.fieldNames()[:-1]: print(\"\\t\", f\"`{fields[field].name}` {fields[field].dataType.simpleString()}, \") last = fields[fields.fieldNames()[-1]] print(\"\\t\", f\"`{last.name}` {last.dataType.simpleString()} \") print(\")\") print(\"STORED AS PARQUET\") print(f\"LOCATION '{s3location}'\\n\")","title":"print_athena_create_table()"},{"location":"reference/functions/dataframe_functions/#functions.dataframe_functions.show_output_to_df","text":"Show output as spark DataFrame Parameters: Name Type Description Default show_output str String representing output of 'show' command in spark required spark SparkSession SparkSession object required Returns: Type Description Dataframe DataFrame object containing output of a show command in spark Source code in functions/dataframe_functions.py def show_output_to_df(show_output: str, spark: SparkSession) -> DataFrame: \"\"\"Show output as spark DataFrame :param show_output: String representing output of 'show' command in spark :type show_output: str :param spark: SparkSession object :type spark: SparkSession :return: DataFrame object containing output of a show command in spark :rtype: Dataframe \"\"\" l = show_output.split(\"\\n\") ugly_column_names = l[1] pretty_column_names = [i.strip() for i in ugly_column_names[1:-1].split(\"|\")] pretty_data = [] ugly_data = l[3:-1] for row in ugly_data: r = [i.strip() for i in row[1:-1].split(\"|\")] pretty_data.append(tuple(r)) return spark.createDataFrame(pretty_data, pretty_column_names)","title":"show_output_to_df()"},{"location":"reference/functions/dataframe_functions/#functions.dataframe_functions.to_list_of_dictionaries","text":"Convert a Spark DataFrame to a list of dictionaries. Parameters: Name Type Description Default df DataFrame The Spark DataFrame to convert. required Returns: Type Description List[Dict[str, Any]] A list of dictionaries representing the rows in the DataFrame. Source code in functions/dataframe_functions.py def to_list_of_dictionaries(df: DataFrame) -> List[Dict[str, Any]]: \"\"\"Convert a Spark DataFrame to a list of dictionaries. :param df: The Spark DataFrame to convert. :type df: :py:class:`pyspark.sql.DataFrame` :return: A list of dictionaries representing the rows in the DataFrame. :rtype: List[Dict[str, Any]] \"\"\" return list(map(lambda r: r.asDict(), df.collect()))","title":"to_list_of_dictionaries()"},{"location":"reference/functions/dataframe_functions/#functions.dataframe_functions.two_columns_to_dictionary","text":"Collect two columns as dictionary when first column is key and second is value. Parameters: Name Type Description Default df DataFrame Input DataFrame required key_col_name str Key-column required value_col_name str Value-column required Returns: Type Description Dict[str, Any] Dictionary with values Source code in functions/dataframe_functions.py def two_columns_to_dictionary( df: DataFrame, key_col_name: str, value_col_name: str ) -> Dict[str, Any]: \"\"\"Collect two columns as dictionary when first column is key and second is value. :param df: Input DataFrame :type df: pyspark.sql.DataFrame :param key_col_name: Key-column :type key_col_name: str :param value_col_name: Value-column :type value_col_name: str :return: Dictionary with values :rtype: Dict[str, Any] \"\"\" k, v = key_col_name, value_col_name return {x[k]: x[v] for x in df.select(k, v).collect()}","title":"two_columns_to_dictionary()"},{"location":"reference/functions/dataframe_validator/","text":"DataFrameMissingColumnError Bases: ValueError raise this when there's a DataFrame column error Source code in functions/dataframe_validator.py class DataFrameMissingColumnError(ValueError): \"\"\"raise this when there's a DataFrame column error\"\"\" DataFrameMissingStructFieldError Bases: ValueError raise this when there's a DataFrame column error Source code in functions/dataframe_validator.py class DataFrameMissingStructFieldError(ValueError): \"\"\"raise this when there's a DataFrame column error\"\"\" DataFrameProhibitedColumnError Bases: ValueError raise this when a DataFrame includes prohibited columns Source code in functions/dataframe_validator.py class DataFrameProhibitedColumnError(ValueError): \"\"\"raise this when a DataFrame includes prohibited columns\"\"\" validate_absence_of_columns(df, prohibited_col_names) Validate that none of the prohibited column names are present among specified DataFrame columns. Parameters: Name Type Description Default df DataFrame DataFrame containing columns to be checked. required prohibited_col_names List [ str ] List of prohibited column names. required Raises: Type Description DataFrameProhibitedColumnError If the prohibited column names are present among the specified DataFrame columns. Source code in functions/dataframe_validator.py def validate_absence_of_columns(df: DataFrame, prohibited_col_names: List[str]) -> None: \"\"\" Validate that none of the prohibited column names are present among specified DataFrame columns. :param df: DataFrame containing columns to be checked. :param prohibited_col_names: List of prohibited column names. :raises DataFrameProhibitedColumnError: If the prohibited column names are present among the specified DataFrame columns. \"\"\" all_col_names = df.columns extra_col_names = [x for x in all_col_names if x in prohibited_col_names] error_message = \"The {extra_col_names} columns are not allowed to be included in the DataFrame with the following columns {all_col_names}\".format( extra_col_names=extra_col_names, all_col_names=all_col_names ) if extra_col_names: raise DataFrameProhibitedColumnError(error_message) validate_presence_of_columns(df, required_col_names) Validates the presence of column names in a DataFrame. Parameters: Name Type Description Default df DataFrame A spark DataFrame. required required_col_names List [ str ] List of the required column names for the DataFrame. required Returns: Type Description None None. Raises: Type Description DataFrameMissingColumnError if any of the requested column names are not present in the DataFrame. Source code in functions/dataframe_validator.py def validate_presence_of_columns(df: DataFrame, required_col_names: List[str]) -> None: \"\"\"Validates the presence of column names in a DataFrame. :param df: A spark DataFrame. :type df: DataFrame` :param required_col_names: List of the required column names for the DataFrame. :type required_col_names: :py:class:`list` of :py:class:`str` :return: None. :raises DataFrameMissingColumnError: if any of the requested column names are not present in the DataFrame. \"\"\" all_col_names = df.columns missing_col_names = [x for x in required_col_names if x not in all_col_names] error_message = \"The {missing_col_names} columns are not included in the DataFrame with the following columns {all_col_names}\".format( missing_col_names=missing_col_names, all_col_names=all_col_names ) if missing_col_names: raise DataFrameMissingColumnError(error_message) validate_schema(df, required_schema, ignore_nullable=False) This function will validate that a given DataFrame has a given StructType as its schema. Parameters: Name Type Description Default df DataFrame DataFrame to validate required required_schema StructType StructType required for the DataFrame required ignore_nullable bool (Optional) A flag for if nullable fields should be ignored during validation False Raises: Type Description DataFrameMissingStructFieldError if any StructFields from the required schema are not included in the DataFrame schema Source code in functions/dataframe_validator.py def validate_schema(df: DataFrame, required_schema: StructType, ignore_nullable: bool=False) -> None: \"\"\" This function will validate that a given DataFrame has a given StructType as its schema. :param df: DataFrame to validate :type df: DataFrame :param required_schema: StructType required for the DataFrame :type required_schema: StructType :param ignore_nullable: (Optional) A flag for if nullable fields should be ignored during validation :type ignore_nullable: bool, optional :raises DataFrameMissingStructFieldError: if any StructFields from the required schema are not included in the DataFrame schema \"\"\" _all_struct_fields = copy.deepcopy(df.schema) _required_schema = copy.deepcopy(required_schema) if ignore_nullable: for x in _all_struct_fields: x.nullable = None for x in _required_schema: x.nullable = None missing_struct_fields = [x for x in _required_schema if x not in _all_struct_fields] error_message = \"The {missing_struct_fields} StructFields are not included in the DataFrame with the following StructFields {all_struct_fields}\".format( missing_struct_fields=missing_struct_fields, all_struct_fields=_all_struct_fields, ) if missing_struct_fields: raise DataFrameMissingStructFieldError(error_message)","title":"Dataframe validator"},{"location":"reference/functions/dataframe_validator/#functions.dataframe_validator.DataFrameMissingColumnError","text":"Bases: ValueError raise this when there's a DataFrame column error Source code in functions/dataframe_validator.py class DataFrameMissingColumnError(ValueError): \"\"\"raise this when there's a DataFrame column error\"\"\"","title":"DataFrameMissingColumnError"},{"location":"reference/functions/dataframe_validator/#functions.dataframe_validator.DataFrameMissingStructFieldError","text":"Bases: ValueError raise this when there's a DataFrame column error Source code in functions/dataframe_validator.py class DataFrameMissingStructFieldError(ValueError): \"\"\"raise this when there's a DataFrame column error\"\"\"","title":"DataFrameMissingStructFieldError"},{"location":"reference/functions/dataframe_validator/#functions.dataframe_validator.DataFrameProhibitedColumnError","text":"Bases: ValueError raise this when a DataFrame includes prohibited columns Source code in functions/dataframe_validator.py class DataFrameProhibitedColumnError(ValueError): \"\"\"raise this when a DataFrame includes prohibited columns\"\"\"","title":"DataFrameProhibitedColumnError"},{"location":"reference/functions/dataframe_validator/#functions.dataframe_validator.validate_absence_of_columns","text":"Validate that none of the prohibited column names are present among specified DataFrame columns. Parameters: Name Type Description Default df DataFrame DataFrame containing columns to be checked. required prohibited_col_names List [ str ] List of prohibited column names. required Raises: Type Description DataFrameProhibitedColumnError If the prohibited column names are present among the specified DataFrame columns. Source code in functions/dataframe_validator.py def validate_absence_of_columns(df: DataFrame, prohibited_col_names: List[str]) -> None: \"\"\" Validate that none of the prohibited column names are present among specified DataFrame columns. :param df: DataFrame containing columns to be checked. :param prohibited_col_names: List of prohibited column names. :raises DataFrameProhibitedColumnError: If the prohibited column names are present among the specified DataFrame columns. \"\"\" all_col_names = df.columns extra_col_names = [x for x in all_col_names if x in prohibited_col_names] error_message = \"The {extra_col_names} columns are not allowed to be included in the DataFrame with the following columns {all_col_names}\".format( extra_col_names=extra_col_names, all_col_names=all_col_names ) if extra_col_names: raise DataFrameProhibitedColumnError(error_message)","title":"validate_absence_of_columns()"},{"location":"reference/functions/dataframe_validator/#functions.dataframe_validator.validate_presence_of_columns","text":"Validates the presence of column names in a DataFrame. Parameters: Name Type Description Default df DataFrame A spark DataFrame. required required_col_names List [ str ] List of the required column names for the DataFrame. required Returns: Type Description None None. Raises: Type Description DataFrameMissingColumnError if any of the requested column names are not present in the DataFrame. Source code in functions/dataframe_validator.py def validate_presence_of_columns(df: DataFrame, required_col_names: List[str]) -> None: \"\"\"Validates the presence of column names in a DataFrame. :param df: A spark DataFrame. :type df: DataFrame` :param required_col_names: List of the required column names for the DataFrame. :type required_col_names: :py:class:`list` of :py:class:`str` :return: None. :raises DataFrameMissingColumnError: if any of the requested column names are not present in the DataFrame. \"\"\" all_col_names = df.columns missing_col_names = [x for x in required_col_names if x not in all_col_names] error_message = \"The {missing_col_names} columns are not included in the DataFrame with the following columns {all_col_names}\".format( missing_col_names=missing_col_names, all_col_names=all_col_names ) if missing_col_names: raise DataFrameMissingColumnError(error_message)","title":"validate_presence_of_columns()"},{"location":"reference/functions/dataframe_validator/#functions.dataframe_validator.validate_schema","text":"This function will validate that a given DataFrame has a given StructType as its schema. Parameters: Name Type Description Default df DataFrame DataFrame to validate required required_schema StructType StructType required for the DataFrame required ignore_nullable bool (Optional) A flag for if nullable fields should be ignored during validation False Raises: Type Description DataFrameMissingStructFieldError if any StructFields from the required schema are not included in the DataFrame schema Source code in functions/dataframe_validator.py def validate_schema(df: DataFrame, required_schema: StructType, ignore_nullable: bool=False) -> None: \"\"\" This function will validate that a given DataFrame has a given StructType as its schema. :param df: DataFrame to validate :type df: DataFrame :param required_schema: StructType required for the DataFrame :type required_schema: StructType :param ignore_nullable: (Optional) A flag for if nullable fields should be ignored during validation :type ignore_nullable: bool, optional :raises DataFrameMissingStructFieldError: if any StructFields from the required schema are not included in the DataFrame schema \"\"\" _all_struct_fields = copy.deepcopy(df.schema) _required_schema = copy.deepcopy(required_schema) if ignore_nullable: for x in _all_struct_fields: x.nullable = None for x in _required_schema: x.nullable = None missing_struct_fields = [x for x in _required_schema if x not in _all_struct_fields] error_message = \"The {missing_struct_fields} StructFields are not included in the DataFrame with the following StructFields {all_struct_fields}\".format( missing_struct_fields=missing_struct_fields, all_struct_fields=_all_struct_fields, ) if missing_struct_fields: raise DataFrameMissingStructFieldError(error_message)","title":"validate_schema()"}]}